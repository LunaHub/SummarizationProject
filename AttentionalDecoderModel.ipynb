{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AttentionalDecoderModel.ipynb","provenance":[{"file_id":"1OmJ-CGbiMs62NS7s2M_kbiCKU1w54k_T","timestamp":1575883954338}],"collapsed_sections":["p-1ullO1DV3m","7YvccedyDOr9","LTTr-7lqDMjn","rmrLuiYtChKi"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5MUX3OyGDYIm","colab_type":"text"},"source":["# Attentional sequence-to-sequence model\n","\n","Code for sequence-to-sequence model with attentional decoder. Uncomment parts to add/remove attention."]},{"cell_type":"markdown","metadata":{"id":"p-1ullO1DV3m","colab_type":"text"},"source":["## Libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1576405389467,"user_tz":-60,"elapsed":5655,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"id":"mxH71XHcrVO6","outputId":"e3987a58-7bae-4fa8-e966-bd21a2af6225","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["pip install num2words"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: num2words in /usr/local/lib/python3.6/dist-packages (0.5.10)\n","Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qzMiAHGKpQyL","colab_type":"code","colab":{}},"source":["# Import libaries:\n","\n","import numpy as np\n","import random\n","import num2words\n","\n","# For plotting:\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Pytorch stuff:\n","import torch\n","from torch.autograd import Variable\n","from torch.nn.parameter import Parameter\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn.init as init\n","from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n","from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n","\n","cuda = torch.cuda.is_available()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7YvccedyDOr9","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1576405389934,"user_tz":-60,"elapsed":6074,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"id":"qBjkEtIJ9r7v","outputId":"2e75c125-f2c4-471c-de75-751d496d45cd","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["# Generate dataset based on maximum sequence length (interval)\n","interval = 5\n","words = [str(i) for i in range(1,interval+1)] + [num2words.num2words(i) for i in range(1,interval+1)]\n","# Vocabulary dictionaries:\n","idx2word = dict(zip(range(len(words)+3),  ['pad_val'] + words + ['SOS'] + ['EOS']))\n","word2idx = dict(zip(['pad_val'] + words + ['SOS'] + ['EOS'] ,range(len(words)+3)))\n","n_char = len(words)+3\n","vocab_size =n_char\n","\n","# Function to converte from number to string:\n","def word2idx_func(input_words):\n","    return [word2idx[i] for i in input_words]\n","def idx2word_func(input_idx):\n","    return [idx2word[i] for i in input_idx]\n","\n","data_size = 50000\n","inputs = []\n","targets = []\n","for i in range(data_size):\n","  # For contiuous sequences:\n","  # length = random.choice(range(1,interval))\n","  # choice = random.choice(range(0,interval-length))\n","  # inputs.append(words[choice:choice+length])\n","  # targets.append(words[interval+choice:interval+choice+length]+['EOS'])\n","  \n","  # For random sequences:\n","    length = random.choice(range(1,interval))\n","    in_data=[]\n","    out_data = []\n","    for j in range(length):\n","        choice = random.choice(range(0,interval))\n","        in_data.append(words[choice])\n","        out_data.append(words[choice+interval])\n","    inputs.append(in_data)\n","    targets.append(out_data+['EOS'])\n","\n","# Partition dataset into training and validation data:\n","from sklearn.model_selection import train_test_split\n","input_data_train, input_data_val, target_data_train, target_data_val = train_test_split(inputs, targets, test_size=0.33, random_state=42)\n","del inputs, targets\n","\n","print(input_data_train[:3])\n","print(target_data_train[:3])\n","print(len(input_data_train))\n","print(len(input_data_val))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[['5', '1', '5'], ['5', '1'], ['4', '4']]\n","[['five', 'one', 'five', 'EOS'], ['five', 'one', 'EOS'], ['four', 'four', 'EOS']]\n","33500\n","16500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LTTr-7lqDMjn","colab_type":"text"},"source":["## Network"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1576405393359,"user_tz":-60,"elapsed":9473,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"id":"3dwKkLyEzkLi","outputId":"0279c77a-3c21-467f-f553-72e27c244190","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["embed_dim = 128\n","hidden_size = 256\n","\n","input_size_enc = n_char\n","hidden_size_enc = hidden_size\n","\n","input_size_dec = hidden_size_enc\n","hidden_size_dec = hidden_size\n","\n","output_size = vocab_size\n","batch_size= 5\n","\n","emb = nn.Embedding(vocab_size, embed_dim)\n","emb.cuda()\n","\n","########## Define Encoder\n","class Encoder(nn.Module):\n","    def __init__(self, hidden_size,batch_size_enc, bidirectional):\n","        super(Encoder, self).__init__()\n","        self.batch_size = batch_size_enc\n","        self.hidden_size = hidden_size\n","        self.bidirectional=bidirectional\n","        \n","        self.num_directions = 1\n","        if bidirectional == True:\n","            self.num_directions = 2\n","        \n","        self.gru = nn.GRU(embed_dim, hidden_size, bidirectional=bidirectional)\n","        \n","    def forward(self, x, hidden): \n","        ls = [len(i) for i in x]\n","        x_padded = torch.nn.utils.rnn.pad_sequence(x)\n","        x_embedded = emb(x_padded)\n","        x = torch.nn.utils.rnn.pack_padded_sequence(x_embedded, ls, False, False)\n","        x = x.cuda()\n","        output_gru, hidden = self.gru(x, hidden)\n","        output_padded,_ = torch.nn.utils.rnn.pad_packed_sequence(output_gru, batch_first=False)\n","        if self.bidirectional == True:\n","            output = output_padded[:, :, :self.hidden_size] + output_padded[:, :, self.hidden_size:]\n","            last_hidden = (hidden[0, :, :] + hidden[1, : ,:]).unsqueeze(0)\n","        else:\n","            output = output_padded\n","            last_hidden = hidden\n","        return output, last_hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(self.num_directions, self.batch_size, self.hidden_size)\n","\n","\n","###### Define Decoder\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size,input_size,batch_size_dec):\n","        super(Decoder, self).__init__()\n","        self.batch_size=batch_size_dec\n","        self.hidden_size = hidden_size\n","        self.dropout_p = 0.1\n","\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(embed_dim, hidden_size)\n","        self.out_lin = nn.Linear(in_features=hidden_size, out_features=output_size)\n","\n","        # For attention:\n","        self.vector_v = nn.Linear(hidden_size,1,bias=False) #weighting after tanh activation (v^t in the paper)\n","        self.w_h = nn.Linear(hidden_size,hidden_size, bias=False) #weights for encoder hidden states (W_h in the paper)\n","        self.w_s = nn.Linear(hidden_size, hidden_size, bias=True) #weights for decoder hidden states (W_s in the paper and the bias represent b_attn)\n","        self.softmax = nn.Softmax(dim=0)  \n","\n","        self.v_lin_1 = nn.Linear(hidden_size*2,hidden_size)\n","        self.v_lin_2 = nn.Linear(hidden_size,output_size)\n","\n","    def forward(self, x, hidden, h_i):   \n","        x = emb(x)\n","        x = F.relu(x)\n","        output, s_t = self.gru(x, hidden)\n","        \n","        ### NO ATTENTION ###\n","        # output = self.out_lin(output)\n","\n","        ### ATTENTION ###\n","        w_h_i = self.w_h(h_i)\n","        w_s_t = self.w_s(s_t)\n","        e_i = self.vector_v(torch.tanh(w_h_i + w_s_t))\n","        att_dist = self.softmax(e_i)\n","        att_dist = att_dist.permute(1,2,0)\n","        h_i = h_i.permute(1,0,2)\n","        h_t = torch.matmul(att_dist,h_i)\n","        h_t=h_t.permute(1,0,2)\n","        temp = torch.cat((s_t,h_t),dim=2).cuda()\n","        p_vocab = self.v_lin_2(self.v_lin_1(temp))\n","        output = p_vocab\n","\n","        return output, s_t\n","\n","    def initHidden(self):\n","        return torch.zeros(1, self.batch_size, self.hidden_size)\n","\n","# Choose the shape of the encoder\n","net_encoder = Encoder(hidden_size = hidden_size_enc, batch_size_enc=batch_size, bidirectional=False)\n","# Choose the shape of the encoder\n","net_decoder = Decoder(input_size = input_size_dec, hidden_size = hidden_size_dec, batch_size_dec=batch_size)\n","\n","if cuda:\n","    net_encoder = net_encoder.cuda()\n","    net_decoder = net_decoder.cuda()\n","\n","print(net_encoder)\n","print(net_decoder)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Encoder(\n","  (gru): GRU(128, 256)\n",")\n","Decoder(\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (gru): GRU(128, 256)\n","  (out_lin): Linear(in_features=256, out_features=13, bias=True)\n","  (vector_v): Linear(in_features=256, out_features=1, bias=False)\n","  (w_h): Linear(in_features=256, out_features=256, bias=False)\n","  (w_s): Linear(in_features=256, out_features=256, bias=True)\n","  (softmax): Softmax(dim=0)\n","  (v_lin_1): Linear(in_features=512, out_features=256, bias=True)\n","  (v_lin_2): Linear(in_features=256, out_features=13, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rmrLuiYtChKi","colab_type":"text"},"source":["## Checking the forward pass"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1576405393362,"user_tz":-60,"elapsed":9453,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"id":"8fK9CGEv1Gp1","outputId":"a3b1e3cc-6072-429c-e88e-1daec37dc481","colab":{"base_uri":"https://localhost:8080/","height":110}},"source":["input_test_batch = input_data_train[:batch_size]\n","target_test_batch = target_data_train[:batch_size]\n","\n","print(input_test_batch)\n","print(target_test_batch)\n","\n","encoder_hidden = net_encoder.initHidden()\n","\n","temp=map(torch.tensor,map(word2idx_func, input_test_batch))\n","print('temp: ',temp)\n","enc_input = list(temp)\n","encoder_input=[]\n","for item in enc_input:\n","    item = item.cuda()\n","    encoder_input.append(item)\n","    \n","outputs_enc, hidden = net_encoder(encoder_input, encoder_hidden.cuda())\n","print('end_encoder')\n","\n","decoder_hidden = hidden\n","decoder_input = torch.tensor([[word2idx['SOS']]*batch_size]).cuda()\n","y, decoder_hidden = net_decoder(decoder_input, decoder_hidden, outputs_enc)\n","new_y=y.view(-1,n_char)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[['5', '1', '5'], ['5', '1'], ['4', '4'], ['5', '2', '5'], ['1', '4', '3', '4']]\n","[['five', 'one', 'five', 'EOS'], ['five', 'one', 'EOS'], ['four', 'four', 'EOS'], ['five', 'two', 'five', 'EOS'], ['one', 'four', 'three', 'four', 'EOS']]\n","temp:  <map object at 0x7f0ed3fff748>\n","end_encoder\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Br3P7LSYCou2","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ig2F7WLIN_lQ","colab":{}},"source":["## Defining loss function and optimizer\n","criterion = nn.CrossEntropyLoss(ignore_index=word2idx['pad_val'])\n","\n","LEARNING_RATE = 0.01\n","optimizer_encoder = optim.SGD(net_encoder.parameters(), lr=LEARNING_RATE)\n","optimizer_decoder = optim.SGD(net_decoder.parameters(), lr=LEARNING_RATE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4Uc2Em5RCGrH","outputId":"76386717-88fc-4ac7-a631-326d03603b5b","executionInfo":{"status":"ok","timestamp":1576405748655,"user_tz":-60,"elapsed":364704,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["epoch = 5\n","total_loss = []\n","N = len(input_data_train)-batch_size\n","n_batches = N/batch_size\n","\n","net_encoder.train()\n","net_decoder.train()\n","\n","for e in range(epoch):\n","  print('Epoch: ' + str(e))\n","\n","  for idx in range(0,N,batch_size):\n","      input_batch_temp = list(map(torch.tensor,map(word2idx_func,input_data_train[idx:idx+batch_size])))\n","      input_batch=[]\n","      for item in input_batch_temp:\n","          item = item.cuda()\n","          input_batch.append(item)\n","      target_batch =list(map(torch.tensor,map(word2idx_func,target_data_train[idx:idx+batch_size])))\n","      target_batch = nn.utils.rnn.pad_sequence(target_batch).cuda()\n","      \n","      loss = 0\n","\n","      encoder_hidden = net_encoder.initHidden().cuda()\n","      output_enc, encoder_hidden = net_encoder(input_batch,encoder_hidden)\n","\n","      padded_tensor_target = target_batch\n","      decoder_hidden = encoder_hidden\n","      decoder_input = torch.tensor([[word2idx['SOS']]*batch_size]).cuda()\n","      for j in range(len(target_batch)):\n","          y_predicted, decoder_hidden = net_decoder(decoder_input,decoder_hidden,output_enc)        \n","          loss += criterion(y_predicted.view(-1,n_char),target_batch[j].type(torch.LongTensor).cuda())\n","          decoder_input = target_batch[j].type(torch.LongTensor).view(1,-1).cuda() #teacher forced\n","\n","      optimizer_encoder.zero_grad()\n","      optimizer_decoder.zero_grad()\n","      loss.backward()\n","      optimizer_encoder.step()\n","      optimizer_decoder.step()\n","\n","      total_loss.append(loss)\n","      if idx % round(N/10) == 0:\n","        print( 'Batch: ' + str( int( idx/round(N/10)) +1 )+ ' of 10. Loss: ' + str(loss) )\n","\n","plt.plot(total_loss);"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch: 0\n","Batch: 1 of 10. Loss: tensor(12.8520, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 2 of 10. Loss: tensor(0.1544, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 3 of 10. Loss: tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 4 of 10. Loss: tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 5 of 10. Loss: tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 6 of 10. Loss: tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 7 of 10. Loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 8 of 10. Loss: tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 9 of 10. Loss: tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 10 of 10. Loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n","Epoch: 1\n","Batch: 1 of 10. Loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 2 of 10. Loss: tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 3 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 4 of 10. Loss: tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 5 of 10. Loss: tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 6 of 10. Loss: tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 7 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 8 of 10. Loss: tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 9 of 10. Loss: tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 10 of 10. Loss: tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n","Epoch: 2\n","Batch: 1 of 10. Loss: tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 2 of 10. Loss: tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 3 of 10. Loss: tensor(9.2761e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 4 of 10. Loss: tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 5 of 10. Loss: tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 6 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 7 of 10. Loss: tensor(9.1743e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 8 of 10. Loss: tensor(7.3560e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 9 of 10. Loss: tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 10 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Epoch: 3\n","Batch: 1 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 2 of 10. Loss: tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 3 of 10. Loss: tensor(5.9954e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 4 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 5 of 10. Loss: tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 6 of 10. Loss: tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 7 of 10. Loss: tensor(6.1512e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 8 of 10. Loss: tensor(5.0926e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 9 of 10. Loss: tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 10 of 10. Loss: tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n","Epoch: 4\n","Batch: 1 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 2 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 3 of 10. Loss: tensor(4.2915e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 4 of 10. Loss: tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 5 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 6 of 10. Loss: tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 7 of 10. Loss: tensor(4.6873e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 8 of 10. Loss: tensor(3.9037e-05, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 9 of 10. Loss: tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch: 10 of 10. Loss: tensor(7.6580e-05, device='cuda:0', grad_fn=<AddBackward0>)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCHKHwAyRPkDQIYofwDI0P8DTCfzza6fcrIAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"hSUZicLeCxnz","colab_type":"text"},"source":["## Validation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aG2gKLPmPxrH","outputId":"37bb47a9-202b-407e-83f7-f13df8a32999","executionInfo":{"status":"ok","timestamp":1576405764456,"user_tz":-60,"elapsed":380483,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["net_encoder.eval()\n","net_decoder.eval()\n","\n","accuracy = 0\n","max_output_char = interval+1\n","N = len(input_data_val)-batch_size\n","print('N: ', N)\n","\n","for idx in range(0,N,batch_size):\n","    if idx % 5000 == 0:\n","        print('idx: ', idx )\n","    input_batch_temp = list(map(torch.tensor,map(word2idx_func,input_data_val[idx:idx+batch_size])))  \n","    input_batch = []\n","    for item in input_batch_temp:\n","        item = item.cuda()\n","        input_batch.append(item)\n","        \n","    target_batch = target_data_val[idx:idx+batch_size]\n","    \n","    encoder_hidden = net_encoder.initHidden().cuda()\n","    output_enc, encoder_hidden = net_encoder(input_batch,encoder_hidden)\n","\n","    # Looping over the i'th element of each batch\n","    decoder_hidden = encoder_hidden\n","    decoder_input = torch.tensor([[word2idx['SOS']]*batch_size]).cuda()\n","    flag = True\n","    counter = 0\n","    predicted_outputs = [[] for i in range(batch_size)]\n","    while counter <= max_output_char and flag:\n","        y_predicted, decoder_hidden = net_decoder(decoder_input,decoder_hidden,output_enc)   \n","        topv, topi = y_predicted.topk(1)\n","        y = topi; decoder_input = y.view(1,-1)\n","        y=y.view(batch_size)\n","        for i in range(batch_size):\n","          # Only append if list is empty or last element of list is not 'EOS'\n","            if not predicted_outputs[i]:             \n","                predicted_outputs[i].append(idx2word[y[i].item()])\n","            elif predicted_outputs[i][-1] != 'EOS':\n","                predicted_outputs[i].append(idx2word[y[i].item()])\n","        if all(x == 'EOS' for x in [predicted_outputs[i][-1] for i in range(batch_size)]):\n","            flag = False\n","        counter +=1\n","    \n","    # For printing wrong predictions:\n","    for idx in range(len(predicted_outputs)):\n","        if predicted_outputs[idx] != target_batch[idx]:\n","            # Only print the wrong ones...\n","            print('Wrong prediction on: ')\n","            print('Input:' + str(input_batch[idx]) + '\\n Target: ' + str(target_batch[idx]) + ',\\n Output: ' + str(predicted_outputs[idx]))\n","  \n","    accuracy += sum([predicted_outputs[i] == target_batch[i] for i in range(batch_size)])\n","\n","print('Accuracy: '+ str(accuracy/N*100) + '%')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["N:  16495\n","idx:  0\n","idx:  5000\n","idx:  10000\n","idx:  15000\n","Accuracy: 100.0%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XPjX4fj6DHg-","colab_type":"text"},"source":["## Summary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J5SpeMdNImPH","outputId":"16ab185e-23b8-46e5-d760-af58fc8eec06","executionInfo":{"status":"ok","timestamp":1576405764458,"user_tz":-60,"elapsed":380463,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["interval"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sRD_FR4eInDi","outputId":"bf4051b9-fdf9-451e-d0f1-1b1c8ee3be16","executionInfo":{"status":"ok","timestamp":1576405764459,"user_tz":-60,"elapsed":380445,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(loss)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor(6.1417e-05, device='cuda:0', grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5mE_WJ4GIniY","outputId":"ebf40feb-902c-4f5f-afea-4ac57dbf6e8b","executionInfo":{"status":"ok","timestamp":1576405764460,"user_tz":-60,"elapsed":380418,"user":{"displayName":"Luna Hansen","photoUrl":"","userId":"10755689247726023676"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print('Accuracy: '+ str(accuracy/N*100) + '%')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Accuracy: 100.0%\n"],"name":"stdout"}]}]}